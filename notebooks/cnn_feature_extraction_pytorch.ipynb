{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries and download pretrained models\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchsummary import summary \n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.vgg16(pretrained=True) #Using VGG-16 as feature extractor\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size = (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove last fully-connected layer\n",
    "new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])\n",
    "model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size = (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Data and setup data\n",
    "# Set directory paths for our files\n",
    "train_dir = './train'\n",
    "test_dir = './test1'\n",
    "\n",
    "# Get files in our directories\n",
    "train_files = os.listdir(train_dir)\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "print(f'Number of images in {train_dir} is {len(train_files)}')\n",
    "print(f'Number of images in {test_dir} is {len(test_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Data Transformations\n",
    "transformations = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataSet Class for our training\n",
    "class Dataset():\n",
    "    def __init__(self, filelist, filepath, transform = None):\n",
    "        self.filelist = filelist\n",
    "        self.filepath = filepath\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.filelist))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath = os.path.join(self.filepath, self.filelist[index])\n",
    "        img = Image.open(imgpath)\n",
    "\n",
    "        if \"dog\" in imgpath:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0 \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our train and test dataset objects\n",
    "train = Dataset(train_files, train_dir, transformations)\n",
    "val = Dataset(test_files, test_dir, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our dataloaders\n",
    "train_dataset = torch.utils.data.DataLoader(dataset = train, batch_size = 32, shuffle=True)\n",
    "val_dataset = torch.utils.data.DataLoader(dataset = val, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train image name and path storing\n",
    "image_names = os.listdir(\"./train\")\n",
    "image_paths = [\"./train/\"+ x for x in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing feature extraction\n",
    "model.eval() \n",
    "model = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = None\n",
    "    image_labels = None\n",
    "\n",
    "    # loop over each batch and pass our input tensors to hte model\n",
    "    for data, label in tqdm.tqdm(train_dataset):\n",
    "        x = data.cuda()\n",
    "        output = model(x)\n",
    "        \n",
    "        if features is not None:\n",
    "            # Concatenates the given sequence of tensors in the given dimension.\n",
    "            # cat needs at least two tensors so we only start to cat after the first loop\n",
    "            features = torch.cat((features, output), 0)\n",
    "            image_labels = torch.cat((image_labels, label), 0)\n",
    "        else:\n",
    "            features = output\n",
    "            image_labels = label\n",
    "\n",
    "    # reshape our tensor to 25000 x 25088 \n",
    "    features = features.view(features.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have features for all images\n",
    "features.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have labels for all images\n",
    "image_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape to ensure our features are a flattened 512*7*7 array\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a LR Classifier using those features\n",
    "# Convert our tensors to numpy arrays\n",
    "features_np = features.cpu().numpy()\n",
    "image_labels_np = image_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split our model into a test and training dataset to train our LR classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_np, image_labels_np, test_size=0.2, random_state = 7)\n",
    "\n",
    "glm = LogisticRegression(C=0.1)\n",
    "glm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accruacy\n",
    "accuracy = glm.score(X_test, y_test)\n",
    "print(f'Accuracy on validation set using Logistic Regression: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running some inferences on our Test Data\n",
    "image_names_test = os.listdir(\"./test1\")\n",
    "image_paths_test = [\"./test1/\"+ x for x in image_names_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "imsize = 224\n",
    "\n",
    "loader = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
    "\n",
    "def image_loader(loader, image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = torch.tensor(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "test_sample = random.sample(image_paths_test, 12)\n",
    "model.eval() \n",
    "\n",
    "def test_img():\n",
    "    result_lst = []\n",
    "    for path in test_sample:\n",
    "      image = image_loader(loader, path)\n",
    "      output = model(image.to(device))\n",
    "      output = output.cpu().detach().numpy() \n",
    "      result = glm.predict(output)\n",
    "      result = 'dog' if float(result) >0.5 else 'cat'\n",
    "      result_lst.append(result)\n",
    "    return result_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions from all models\n",
    "pred_results = test_img()\n",
    "pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "import cv2\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(0, 12):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    result = pred_results[i]\n",
    "    img = test_sample[i]\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_CUBIC)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.text(72, 248, f'Feature Extractor CNN: {result}', color='lightgreen',fontsize= 12, bbox=dict(facecolor='black', alpha=0.9))\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
